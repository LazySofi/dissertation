{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import choice, randrange\n",
    "from datetime import  date, timedelta, datetime\n",
    "from sqlalchemy import func, asc\n",
    "\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mpl_colors\n",
    "from mpl_toolkits.basemap import Basemap as Basemap\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.forecasting.stl import STLForecast\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.metrics import  mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from scipy.stats import kstest, ttest_ind\n",
    "from scipy.signal import periodogram\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "\n",
    "from lib_database import *\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.options.display.expand_frame_repr = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ судов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ распределения судов по бассейнам судов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def various_bassin(df, min_calls=0, ship_type ='13 / Oil tanker', print_table=False, print_hist=False):\n",
    "    df_pivot = pd.pivot_table(df[(df['Количество судозаходов']>2) & (df['Тип судна']=='13 / Oil tanker')],\n",
    "                                index=['ship_id', 'Тип судна', 'Бассейн'],)\n",
    "    \n",
    "    if print_table:\n",
    "        print(df_pivot)\n",
    "\n",
    "    df_pivot = pd.pivot_table(df[(df['Количество судозаходов']>min_calls) & (df['Тип судна']==ship_type)],\n",
    "                                index=['ship_id', 'Тип судна'],\n",
    "                                values=['Бассейн'],\n",
    "                                aggfunc=[len]\n",
    "                                )\n",
    "    if print_hist:\n",
    "        df_pivot.hist()\n",
    "    \n",
    "    return df_pivot.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(engine) as session:\n",
    "    query = session.query(\n",
    "                        PortCall.ship_id, \n",
    "                        Ship.type,\n",
    "                        Port.basin,\n",
    "                        func.count(PortCall.arrival).label('Количество судозаходов')\n",
    "                        ).\\\n",
    "                    join(Port, PortCall.port_id == Port.id).\\\n",
    "                    join(Ship, PortCall.ship_id == Ship.id).\\\n",
    "                    group_by(PortCall.ship_id,  Ship.type, Port.basin)\n",
    "\n",
    "    df = pd.read_sql(query.statement, query.session.bind)\n",
    "\n",
    "ship_types = df['Тип судна'].drop_duplicates().dropna().to_list()\n",
    "rows = {'Тип судна': [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: []}\n",
    "for st in ship_types:\n",
    "    r = various_bassin(df, ship_type=st)\n",
    "    rows['Тип судна'].append(st)\n",
    "\n",
    "    for i in range(6):\n",
    "        try:\n",
    "            rows[i+1].append(r[i+1])\n",
    "        except:\n",
    "            rows[i+1].append(np.NaN)\n",
    "\n",
    "df_various_bassin= pd.DataFrame(rows)\n",
    "df_various_bassin.to_excel('Вариативность_бассейнов_по_типам_судов.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка распределений возраста и тоннажности судов (рис 7 и 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_histogram(date_from, date_to, port, ship_type):\n",
    "    with Session(engine) as session:\n",
    "        query = session.query((date.today().year - Ship.year).label('Возраст')).\\\n",
    "                        join(PortCall, PortCall.ship_id == Ship.id).\\\n",
    "                        join(Port, PortCall.port_id == Port.id).\\\n",
    "                        filter(Port.name.in_(tuple(port))).\\\n",
    "                        filter(Ship.type.in_(tuple(ship_type))).\\\n",
    "                        filter(Ship.year != None).\\\n",
    "                        filter(Ship.year != 0).\\\n",
    "                        filter(PortCall.ship_id != None).\\\n",
    "                        filter(PortCall.arrival >= date_from).\\\n",
    "                        filter(PortCall.arrival <= date_to)\n",
    "\n",
    "        df = pd.read_sql(query.statement, query.session.bind)\n",
    "    fig = go.Figure(go.Histogram(x=df['Возраст']))\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=20, b=20),\n",
    "        xaxis_title=\"возраст корабля\",\n",
    "        yaxis_title=\"частота заданного возраста\", \n",
    "        )\n",
    "    \n",
    "    return fig, df\n",
    "\n",
    "def tonnage_histogram(date_from, date_to, port, ship_type):\n",
    "    with Session(engine) as session:\n",
    "        query = session.query(Ship.tonnage).\\\n",
    "                        join(PortCall, PortCall.ship_id == Ship.id).\\\n",
    "                        join(Port, PortCall.port_id == Port.id).\\\n",
    "                        filter(Port.name ==port).\\\n",
    "                        filter(Ship.type.in_(tuple(ship_type))).\\\n",
    "                        filter(Ship.tonnage != None).\\\n",
    "                        filter(Ship.tonnage != 0).\\\n",
    "                        filter(PortCall.ship_id != None).\\\n",
    "                        filter(PortCall.arrival >= date_from).\\\n",
    "                        filter(PortCall.arrival <= date_to)\n",
    "\n",
    "        df = pd.read_sql(query.statement, query.session.bind)\n",
    "\n",
    "    fig = go.Figure(go.Histogram(x=df['Регист. вместим. валовая [т]']))\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=20, b=20),\n",
    "        xaxis_title=\"Регист. вместим. валовая [т]\",\n",
    "        yaxis_title=\"частота заданной вместимости\", \n",
    "        )\n",
    "    \n",
    "    return fig, df\n",
    "\n",
    "def get_best_distribution(data, print_info = False):\n",
    "    dist_names = [\"norm\", \"exponweib\", \"weibull_max\", \"weibull_min\", \"pareto\", \"genextreme\"]\n",
    "    dist_results = []\n",
    "    params = {}\n",
    "    for dist_name in dist_names:\n",
    "        dist = getattr(st, dist_name)\n",
    "        param = dist.fit(data)\n",
    "\n",
    "        params[dist_name] = param\n",
    "        D, p = st.kstest(data, dist_name, args=param)\n",
    "        if print_info:\n",
    "            print(\"p value for \"+dist_name+\" = \"+str(p))\n",
    "        dist_results.append((dist_name, p))\n",
    "\n",
    "    # select the best fitted distribution\n",
    "    best_dist, best_p = (max(dist_results, key=lambda item: item[1]))\n",
    "    # store the name of the best fit and its p value\n",
    "\n",
    "    if print_info:\n",
    "        print(\"Best fitting distribution: \"+str(best_dist))\n",
    "        print(\"Best p value: \"+ str(best_p))\n",
    "        print(\"Parameters for the best fit: \"+ str(params[best_dist]))\n",
    "\n",
    "    return best_dist, best_p, params[best_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(engine) as session:\n",
    "        query = session.query(Port.name, Port.basin)\n",
    "        port_df = pd.read_sql(query.statement, query.session.bind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возраст\n",
    "port_df['best_distribution'] = '-'\n",
    "port_df['p'] = '-'\n",
    "port_df['params_best_dist'] = '-'\n",
    "\n",
    "date_from=date(2021, 1, 1) \n",
    "date_to=date(2021, 12, 31)\n",
    "ship_type=['60 / General cargo/multi-purpose ship']\n",
    "for port_name in port_df['Название порта']:\n",
    "    fig, df = age_histogram(date_from, date_to, [port_name], ship_type)\n",
    "    if len(df)>10:\n",
    "        best_dist, best_p, params_best_dist = get_best_distribution(df['Возраст'])\n",
    "        port_df.loc[port_df['Название порта'] == port_name, 'best_distribution'] = best_dist\n",
    "        port_df.loc[port_df['Название порта'] == port_name, 'p'] = best_p\n",
    "        port_df.loc[port_df['Название порта'] == port_name, 'params_best_dist'] = '(' +', '.join(map(str, np.round(params_best_dist, 2))) + ')'\n",
    "\n",
    "a = port_df[(port_df['best_distribution'] !='-')]\n",
    "a[a['p']>0.01].sort_values(['Бассейн', 'best_distribution', 'p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тоннаж\n",
    "port_df['best_distribution'] = '-'\n",
    "port_df['p'] = '-'\n",
    "port_df['params_best_dist'] = '-'\n",
    "\n",
    "date_from=date(2021, 1, 1) \n",
    "date_to=date(2021, 12, 31)\n",
    "ship_type=['60 / General cargo/multi-purpose ship']\n",
    "for port_name in port_df['Название порта']:\n",
    "    fig, df = tonnage_histogram(date_from, date_to, port_name, ship_type)\n",
    "    if len(df)>10:\n",
    "        best_dist, best_p, params_best_dist = get_best_distribution(df['Регист. вместим. валовая [т]'])\n",
    "        port_df.loc[port_df['Название порта'] == port_name, 'best_distribution'] = best_dist\n",
    "        port_df.loc[port_df['Название порта'] == port_name, 'p'] = best_p\n",
    "        port_df.loc[port_df['Название порта'] == port_name, 'params_best_dist'] = '(' +', '.join(map(str, np.round(params_best_dist, 2))) + ')'\n",
    "\n",
    "a = port_df[(port_df['best_distribution'] !='-')]\n",
    "a[a['p']>0.01].sort_values(['Бассейн', 'best_distribution', 'p'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Графики показывающий зависимость длины, ширины судна, а также его типа, а также графики зависимости времени пребывания от регистрируемой валовой вместимости (рис 9-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 'Азов'\n",
    "with Session(engine) as session:\n",
    "        query = session.query(PortCall.arrival, PortCall.departure, Ship.tonnage, Ship.type, Ship.length, Ship.width).\\\n",
    "                        join(PortCall, PortCall.ship_id == Ship.id).\\\n",
    "                        join(Port, PortCall.port_id == Port.id).\\\n",
    "                        filter(Port.name == port).\\\n",
    "                        filter(PortCall.arrival != None).\\\n",
    "                        filter(PortCall.departure != None)\n",
    "        df = pd.read_sql(query.statement, query.session.bind)\n",
    "df=df.dropna()\n",
    "df['Время пребывания [ч]'] = (df['Дата/время отхода'] - df['Дата/время захода']).dt.total_seconds() / 3600\n",
    "df['Тип судна (кратко)'] = df['Тип судна'].apply(lambda x: x.split('/')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='Регист. вместим. валовая [т]', y= 'Время пребывания [ч]', color='Тип судна (кратко)', size='Длина наибольшая [м]')\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(df, x='Длина наибольшая [м]', y= 'Ширина наибольшая [м]', color='Тип судна (кратко)')\n",
    "fig.show()\n",
    "\n",
    "df_b = df[df['Тип судна (кратко)'].isin(['60', '13', '85', '14'])]\n",
    "fig = px.scatter(df_b, x='Длина наибольшая [м]', y= 'Ширина наибольшая [м]', color='Тип судна (кратко)')\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(df_b, x='Регист. вместим. валовая [т]',  y='Время пребывания [ч]', color='Тип судна (кратко)', size='Длина наибольшая [м]')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ портов как графа"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Графическое представление системы портов. Ребра построены исходя из данных за период 01.10.2022-31.12.2022 по всем типам судов. Вес ребра изображен его толщиной. Цветом обозначены бассейны (рис 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(engine) as session:\n",
    "        query = session.query(Port.name, Port.basin, Port.lat, Port.lon).\\\n",
    "                        order_by(Port.id)\n",
    "        port_df = pd.read_sql(query.statement, query.session.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_table = np.zeros((107, 107), dtype=int) # порт отправления, порт прибытия\n",
    "ships = dict() # корабль, нынешний порт\n",
    "\n",
    "date_from=date(2022, 10, 1) \n",
    "date_to=date(2022, 12, 31)\n",
    "\n",
    "with Session(engine) as session:\n",
    "        query = session.query(PortCall.ship_id, PortCall.port_id).\\\n",
    "                        filter(PortCall.arrival >= date_from).\\\n",
    "                        filter(PortCall.arrival <= date_to).\\\n",
    "                        order_by(PortCall.arrival.asc())\n",
    "        \n",
    "        for v in query:\n",
    "            if v[0] in ships.keys():\n",
    "                adj_table[ships[v[0]], v[1]] += 1\n",
    "            ships[v[0]] = v[1]\n",
    "                        \n",
    "np.sum(adj_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "nodes = [(node, {'name': attr['Название порта'], 'basin': attr['Бассейн']}) for (node, attr) in port_df.to_dict('index').items()]\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "edges = []\n",
    "for i in range(107):\n",
    "    for j in range(107):\n",
    "        if adj_table[i, j] !=0:\n",
    "            G.add_edge(i, j, weight=adj_table[i, j])\n",
    "\n",
    "\n",
    "nx.draw(G, node_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 9))\n",
    "\n",
    "m = Basemap(\n",
    "    projection='merc', #mill merc\n",
    "    llcrnrlat=30,\n",
    "    urcrnrlat=85,\n",
    "    llcrnrlon=-10,\n",
    "    urcrnrlon=200,\n",
    "    resolution='c',\n",
    "    )\n",
    "\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "# m.fillcontinents(color='coral',lake_color='aqua')\n",
    "# m.drawmapboundary(fill_color='aqua')\n",
    "\n",
    "#зададим положение, цвет и подпись для вершины графа (порта)\n",
    "basin_color = {\n",
    "    'Азово-Черноморский бассейн': 'olivedrab', \n",
    "    'Дальневосточный бассейн': 'dodgerblue',\n",
    "    'Балтийский бассейн': 'firebrick', \n",
    "    'Арктический бассейн': 'aquamarine', \n",
    "    'Каспийский бассейн': 'blueviolet', \n",
    "    'Беломорско-Онежский бассейн': 'orange', \n",
    "    'Другое': 'pink'}\n",
    "pos = dict()\n",
    "node_labels = dict()\n",
    "node_colors = []\n",
    "for index, row in port_df.iterrows():\n",
    "    pos[index] = m(row['lon'], row['lat'])\n",
    "    node_labels[index] = row['Название порта']\n",
    "    node_colors.append(basin_color[row['Бассейн']])\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_size=20, node_color=node_colors)\n",
    "nx.draw_networkx_labels(G,pos, font_size=3, labels=node_labels, horizontalalignment='right', verticalalignment='top')\n",
    "\n",
    "# зададим толщину ребер графа \n",
    "n_nodes = G.number_of_nodes()\n",
    "all_weights = []\n",
    "for (node1,node2,data) in G.edges(data=True):\n",
    "        all_weights.append(data['weight']) \n",
    "unique_weights = list(set(all_weights))\n",
    "\n",
    "for weight in unique_weights:\n",
    "        weighted_edges = [(node1,node2) for (node1,node2,edge_attr) in G.edges(data=True) if edge_attr['weight']==weight]\n",
    "        width = np.log(weight)*n_nodes*10/sum(all_weights)\n",
    "        nx.draw_networkx_edges(G,pos,edgelist=weighted_edges,width=width, arrowstyle='->')\n",
    "\n",
    "plt.title('Морские порты Российской Федерации')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты исследования основных характеристик сети портов (рис 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_radius_diametr(G, n_of_nodes = 300):\n",
    "    G = nx.Graph(G)\n",
    "    max_component = list(max(nx.connected_components(G), key=len))\n",
    "    max_component_len = len(max_component)\n",
    "\n",
    "    # N случайных вершин\n",
    "    max_component_cut = set()\n",
    "    i=0\n",
    "    while i < n_of_nodes and i < max_component_len:\n",
    "        max_component_cut.add(choice(max_component))\n",
    "        i = len(max_component_cut)\n",
    "    max_component_cut = list(max_component_cut)\n",
    "\n",
    "    distance = []      # Расстояние (геодезическое)\n",
    "    eccentricity = []  # Эксцентриситет\n",
    "    for u in max_component_cut:\n",
    "        d = [nx.shortest_path_length(G, source=u, target=v) for v in max_component_cut] \n",
    "        eccentricity.append(max(d))\n",
    "        distance += d\n",
    "\n",
    "    radius = min(eccentricity)   # Радиус\n",
    "    diameter = max(eccentricity) # Диаметр\n",
    "    percentile = np.percentile(distance, 90) # 90 процентиль расстояния между вершинами графа (в 90% случаев расстояние между вершинами <= этому значению)\n",
    "\n",
    "    print(f'Для наибольшей компоненты слабой связности оценены')\n",
    "    print(f'Радиус: {radius}')\n",
    "    print(f'Диаметр: {diameter}')\n",
    "    print(f'90 процентиль расстояния между вершинами графа: {round(percentile, 2)}')\n",
    "\n",
    "def task_triangles_clustering(G):\n",
    "\n",
    "    G = nx.Graph(G) # приведен к неориентированному\n",
    "\n",
    "    triangles = sum(nx.triangles(G).values())/3\n",
    "    average_clustering = nx.average_clustering(G)\n",
    "    transitivity = nx.transitivity(G)\n",
    "\n",
    "    print(f'Число треугольников в сети: {round(triangles)}')\n",
    "    print(f'Средний кластерный коэффииент: {round(average_clustering, 2)}')\n",
    "    print(f'Глобальный кластерный коэфициент: {round(transitivity, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_nodes = G.number_of_nodes()\n",
    "n_of_edges = G.number_of_edges()\n",
    "density =  np.mean([degree[1] for degree in G.degree()]) / (n_of_nodes - 1)\n",
    "\n",
    "\n",
    "n_weak = nx.number_weakly_connected_components(G)\n",
    "largest_len = len(max(nx.weakly_connected_components(G), key=len))\n",
    "\n",
    "print(f'Число вершин: {n_of_nodes}')\n",
    "print(f'Число рёбер: {n_of_edges}')\n",
    "print(f'Плотность: {round(density, 4)}')\n",
    "print(f'Число компонент слабой связности: {n_weak}')\n",
    "print(f'Доля вершин в максимальной по мощности компоненте слабой связности: {round(largest_len / n_of_nodes, 2)}')\n",
    "print(f'Число компонент сильной связности: {nx.number_strongly_connected_components(G)}')\n",
    "print(f'Доля вершин в максимальной по мощности компоненте сильной связности: {round(len(max(nx.strongly_connected_components(G), key=len))/ n_of_nodes, 2)}')\n",
    "\n",
    "\n",
    "deg_avg = np.mean([degree[1] for degree in G.degree()])\n",
    "count = len([node for node in G.nodes if G.degree(node) > deg_avg])\n",
    "\n",
    "print(f'Число вершин степени, превышающей среднюю степень ({round(deg_avg, 2)}): {count}')\n",
    "\n",
    "print()\n",
    "task_radius_diametr(G)\n",
    "\n",
    "print()\n",
    "task_triangles_clustering(G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Графики плотности распределения (рис 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_pdf(G, nbinsx = 20):\n",
    "\n",
    "    node_degrees = [degree[1] for degree in G.degree()]\n",
    "\n",
    "    \n",
    "    fig = go.Figure(data=[go.Histogram(x=node_degrees, nbinsx=nbinsx)])\n",
    "    fig.update_layout(\n",
    "        title='PDF в обычных шкалах',\n",
    "        xaxis_title_text='Степень узла',\n",
    "        yaxis_title_text='Частота',\n",
    "        bargap=0.2,\n",
    "        bargroupgap=0.1,\n",
    "        margin=dict(t=30, b=10, l=10, r=10, pad=0)\n",
    "    )\n",
    "\n",
    "    deg_max = np.max(node_degrees)\n",
    "    X = np.arange(deg_max + 1)\n",
    "    Y = [len([node for node in G.nodes if G.degree(node) == i]) for i in X]\n",
    "    fig_log = go.Figure(go.Scatter(mode=\"markers\", x=X, y=Y))\n",
    "    fig_log.update_xaxes(type=\"log\")\n",
    "    fig_log.update_yaxes(type=\"log\")\n",
    "    fig_log.update_layout(\n",
    "        title='PDF в log - log шкалах',\n",
    "        xaxis_title_text='Степень узла',\n",
    "        yaxis_title_text='Частота',\n",
    "        bargap=0.2,\n",
    "        bargroupgap=0.1,\n",
    "        margin=dict(t=30, b=10, l=10, r=10, pad=0)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    fig_log.show()\n",
    "\n",
    "task_pdf(G, None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доля вершин в максимальной по мощности компоненте слабой связности после удаления некоторого % узлов (таблица 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_delate(G, x = 0.2):\n",
    "    number_of_nodes = G.number_of_nodes()        \n",
    "    G_deleted_n = round(number_of_nodes * x)\n",
    "\n",
    "    print(f'1. Удалены случайным образом {round(x * 100, 2)}% узлов (кол-во: {G_deleted_n})')\n",
    "    deleted_names = []\n",
    "    node_names = [degree[0] for degree in G.degree()]\n",
    "\n",
    "    while len(deleted_names) < G_deleted_n:\n",
    "        deleted_n = randrange(len(node_names))\n",
    "        deleted_names.append(node_names[deleted_n])\n",
    "        node_names.pop(deleted_n)\n",
    "\n",
    "    G_rand = G.copy()\n",
    "    G_rand.remove_nodes_from(deleted_names)\n",
    "    largest_len_rand = len(max(nx.weakly_connected_components(G_rand), key=len)) if nx.is_directed(G) else len(max(nx.connected_components(G_rand), key=len))\n",
    "    print(f'Доля вершин в максимальной по мощности компоненте слабой связности: {round(largest_len_rand / G_rand.number_of_nodes(), 2)}')\n",
    "\n",
    "    print(f'2. Удалены {round(x * 100, 1)}% узлов наибольшей степени (кол-во: {G_deleted_n})')\n",
    "    nodes_sorted = sorted([degree for degree in G.degree()], key=lambda tup: tup[1], reverse=True)\n",
    "    deleted_names = [nodes_sorted[i][0] for i in range(G_deleted_n)]\n",
    "\n",
    "    G_sorted = G.copy()\n",
    "    G_sorted.remove_nodes_from(deleted_names)\n",
    "    largest_len_sorted = len(max(nx.weakly_connected_components(G_sorted), key=len)) if nx.is_directed(G) else len(max(nx.connected_components(G_sorted), key=len))\n",
    "    print(f'Доля вершин в максимальной по мощности компоненте слабой связности: {round(largest_len_sorted / G_sorted.number_of_nodes(), 2)}')\n",
    "for p in range(10):\n",
    "    task_delate(G, p/10)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики центральности (таблица 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_metric_data(G):\n",
    "\n",
    "    max_component = max(nx.weakly_connected_components(G), key=len)\n",
    "    \n",
    "    G_max_component = nx.Graph()            # граф макс компоненты слабой связности\n",
    "    for node in max_component:              # добавим вершины\n",
    "        G_max_component.add_node(node)\n",
    "    for (i, j) in list(G.edges):            # добавляем вершины\n",
    "        if i in max_component and j in max_component:\n",
    "            G_max_component.add_edge(i, j)\n",
    "    \n",
    "    G = nx.Graph(G_max_component)           # наибольшая компонента слабой связности, принятая за неориентированный граф\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def task_centrality(G, delta = 0.2):\n",
    "    # https://arxiv.org/pdf/1608.05845.pdf -- формулы\n",
    "\n",
    "    degree_centrality = nx.centrality.degree_centrality(G)\n",
    "    closeness_centrality = nx.centrality.closeness_centrality(G)\n",
    "    betweenness_centrality = nx.centrality.betweenness_centrality(G)\n",
    "    eigenvector_centrality = nx.centrality.eigenvector_centrality(G)\n",
    "\n",
    "    # decay centrality\n",
    "    # http://papers.econ.ucy.ac.cy/RePEc/papers/04-16.pdf -- формула\n",
    "    decay_centrality = {}\n",
    "    for i in list(G.nodes()):\n",
    "        decay_centrality[i] = 0\n",
    "        for j in list(G.nodes()):\n",
    "            if i != j:\n",
    "                distance = nx.shortest_path_length(G, source = i, target = j)\n",
    "                decay_centrality[i] += delta ** distance\n",
    "\n",
    "    return degree_centrality, closeness_centrality, betweenness_centrality, eigenvector_centrality, decay_centrality\n",
    "\n",
    "    \n",
    "def task_pagerank_hits(G):\n",
    "\n",
    "    pagerank = nx.pagerank(G)\n",
    "    hubs, authorities = nx.hits(G)\n",
    "\n",
    "    return pagerank, hubs, authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_m = task_metric_data(G)\n",
    "\n",
    "delta = 0.2\n",
    "degree_cent, closeness_cent, betweenness_cent, eigenvector_cent, decay_cent = task_centrality(G_m, delta=delta)\n",
    "\n",
    "pagerank, hubs, authorities = task_pagerank_hits(G_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {0: ['degree cenrality', degree_cent], \n",
    "           1: ['closeness cenrality', closeness_cent], \n",
    "           2: ['betweenness cenrality', betweenness_cent], \n",
    "           3: ['eigenvector cenrality', eigenvector_cent], \n",
    "           4: [f'decay cenrality (delta = {delta})', decay_cent, delta], \n",
    "           5: ['pagerank', pagerank], \n",
    "           6: ['hubs', hubs],\n",
    "           7: ['authorities', authorities]\n",
    "}\n",
    "\n",
    "metric = metrics[6]\n",
    "\n",
    "metrics_sorted = sorted(metric[1].items(), key=lambda item: item[1], reverse=True)\n",
    "first10 = pd.DataFrame(metrics_sorted[:10], columns=['Порт', metric[0]])\n",
    "first10[metric[0]] = round(first10[metric[0]], 4)\n",
    "first10['Порт'] = first10.apply(lambda x: port_df.at[x['Порт'], 'Название порта'], axis=1)\n",
    "print(first10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_m = metric[1]\n",
    "for i in (set(range(107)) - set(metric[1].keys())):\n",
    "    g_m[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 9))\n",
    "\n",
    "m = Basemap(\n",
    "    projection='merc', #mill merc\n",
    "    llcrnrlat=30,\n",
    "    urcrnrlat=85,\n",
    "    llcrnrlon=-10,\n",
    "    urcrnrlon=200,\n",
    "    resolution='c',\n",
    "    )\n",
    "\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "\n",
    "#зададим положение, цвет и подпись для вершины графа (порта)\n",
    "pos = dict()\n",
    "node_labels = dict()\n",
    "node_sizes = []\n",
    "for index, row in port_df.iterrows():\n",
    "    pos[index] = m(row['lon'], row['lat'])\n",
    "    node_labels[index] = row['Название порта']\n",
    "\n",
    "\n",
    "    node_sizes.append(g_m[index])\n",
    "\n",
    "node_sizes = np.array(node_sizes)\n",
    "node_sizes = (((node_sizes - node_sizes.min()) / (node_sizes.max() - node_sizes.min())) * 10) ** 3\n",
    "\n",
    "colors = range(len(set(node_sizes)))\n",
    "cmap = plt.cm.viridis #plt.cm.Blues\n",
    "bounds = sorted(list(set(node_sizes)))\n",
    "norm = mpl_colors.BoundaryNorm(bounds, cmap.N, extend='both')\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm._A = []\n",
    "plt.colorbar(sm)\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color = node_sizes, cmap=cmap)\n",
    "nx.draw_networkx_labels(G,pos, font_size=3, labels=node_labels, horizontalalignment='right', verticalalignment='top')\n",
    "\n",
    "# зададим толщину ребер графа \n",
    "n_nodes = G.number_of_nodes()\n",
    "all_weights = []\n",
    "for (node1,node2,data) in G.edges(data=True):\n",
    "        all_weights.append(data['weight']) \n",
    "unique_weights = list(set(all_weights))\n",
    "\n",
    "for weight in unique_weights:\n",
    "        weighted_edges = [(node1,node2) for (node1,node2,edge_attr) in G.edges(data=True) if edge_attr['weight']==weight]\n",
    "        width = np.log(weight)*n_nodes*10/sum(all_weights)\n",
    "        nx.draw_networkx_edges(G,pos,edgelist=weighted_edges,width=width, arrowstyle='->')\n",
    "\n",
    "plt.title(f'Морские порты Российской Федерации ({metric[0]})')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Максимальная клика (полный подграф Kn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Максимальная клика (полный подграф Kn). На графе выделены вершины, принадлежащие максимальной клике\n",
    "def max_clique(G, port_df):\n",
    "    G = nx.Graph(G)\n",
    "    all_cliques = list(nx.find_cliques(G))\n",
    "    max_clique = sorted(all_cliques, key=len, reverse=True)[0]\n",
    "    max_clique_names = [f\"{port_df.iloc[i]['Название порта']} ({port_df.iloc[i]['Бассейн']})\" for i in max_clique]\n",
    "    print(f'Максимальная клика (размер: {len(max_clique)}): {max_clique_names}')\n",
    "\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    m = Basemap(\n",
    "        projection='merc', #mill merc\n",
    "        llcrnrlat=37,\n",
    "        urcrnrlat=62,\n",
    "        llcrnrlon=120,\n",
    "        urcrnrlon=180,\n",
    "        resolution='c',\n",
    "        )\n",
    "\n",
    "    m.drawcoastlines()\n",
    "    m.drawcountries()\n",
    "\n",
    "    #зададим положение, цвет и подпись для вершины графа (порта)\n",
    "    horizontalalignment = {\n",
    "        'Владивосток' : 'right',\n",
    "        'Находка': 'left',\n",
    "        'Ванино': 'center', \n",
    "        'Восточный':'left', \n",
    "        'Корсаков': 'center', \n",
    "        'Славянка': 'right',\n",
    "        'Петропавловск-Камчатский': 'center',\n",
    "        'Магадан': 'center'\n",
    "        }\n",
    "\n",
    "    verticalalignment = {\n",
    "        'Владивосток' : 'bottom',\n",
    "        'Находка': 'bottom',\n",
    "        'Ванино': 'center', \n",
    "        'Восточный':'top', \n",
    "        'Корсаков': 'center', \n",
    "        'Славянка': 'center',\n",
    "        'Петропавловск-Камчатский': 'center',\n",
    "        'Магадан': 'center'\n",
    "        }\n",
    "\n",
    "    pos = dict()\n",
    "    node_labels = dict()\n",
    "    node_colors = ['olivedrab' if node in max_clique else 'aquamarine' for node in G.nodes()]\n",
    "    node_sizes = [100 if node in max_clique else 0 for node in G.nodes()]\n",
    "    font_sizes = dict(G.degree)\n",
    "    for node in G.nodes():\n",
    "        if node in max_clique:\n",
    "            font_sizes[node] = 7\n",
    "        else:\n",
    "            font_sizes[node] = 0\n",
    "    for index, row in port_df.iterrows():\n",
    "        pos[index] = m(row['lon'], row['lat'])\n",
    "        node_labels[index] = row['Название порта']\n",
    "        \n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors)\n",
    "    for node, (x, y) in pos.items():\n",
    "        if node in max_clique:\n",
    "            plt.text(x, y, node_labels[node], \n",
    "                     fontsize=font_sizes[node], \n",
    "                     ha=horizontalalignment[node_labels[node]], \n",
    "                     va=verticalalignment[node_labels[node]], \n",
    "                     weight='bold',\n",
    "                     bbox=dict(boxstyle=\"square\",\n",
    "                                ec=(1., 1.,  1.),\n",
    "                                fc=(0.9, 0.9, 0.9),\n",
    "                                alpha=0.5\n",
    "                                )\n",
    "                     )\n",
    "\n",
    "    plt.title('Максимальная клика на графе морских портов РФ')\n",
    "    plt.show()\n",
    "\n",
    "max_clique(G, port_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сообщества в графе (рис 16-17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communities(G, partitions):\n",
    "    print(f'Количество сообществ: {len(partitions)}')\n",
    "    print(f'Значение метрики модулярности: {round(nx_comm.modularity(G, partitions), 4)}')\n",
    "\n",
    "# максимизация метрики модулярности \n",
    "max_modularity_communities = nx_comm.greedy_modularity_communities(G_m)\n",
    "communities(G_m, max_modularity_communities)\n",
    "\n",
    "# на основе метрики edge betweeness\n",
    "edge_betweenness = nx_comm.girvan_newman(G_m)\n",
    "edge_betweenness = list(sorted(c) for c in next(edge_betweenness))\n",
    "communities(G_m, edge_betweenness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_comm = {0: 'olivedrab', 1: 'dodgerblue', 2: 'firebrick', 3: 'aquamarine', 4: 'blueviolet'}\n",
    "comm_partition = edge_betweenness #edge_betweenness max_modularity_communities\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "\n",
    "m = Basemap(\n",
    "    projection='merc', #mill merc\n",
    "    llcrnrlat=30,\n",
    "    urcrnrlat=85,\n",
    "    llcrnrlon=-10,\n",
    "    urcrnrlon=200,\n",
    "    resolution='c',\n",
    "    )\n",
    "\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "# m.fillcontinents(color='coral',lake_color='aqua')\n",
    "# m.drawmapboundary(fill_color='aqua')\n",
    "\n",
    "#зададим положение, цвет и подпись для вершины графа (порта)\n",
    "pos = dict()\n",
    "node_labels = dict()\n",
    "node_colors = []\n",
    "for index, row in port_df.iterrows():\n",
    "    pos[index] = m(row['lon'], row['lat'])\n",
    "    node_labels[index] = row['Название порта']\n",
    "\n",
    "    comm_flag = False\n",
    "    for i_comm in range(len(comm_partition)):\n",
    "        if index in comm_partition[i_comm]:\n",
    "            node_colors.append(color_comm[i_comm])\n",
    "            comm_flag = True\n",
    "            break\n",
    "    if not comm_flag:\n",
    "        node_colors.append(color_comm[4])\n",
    "    \n",
    "nx.draw_networkx_nodes(G, pos, node_size=20, node_color=node_colors)\n",
    "nx.draw_networkx_labels(G,pos, font_size=3, labels=node_labels, horizontalalignment='right', verticalalignment='top')\n",
    "\n",
    "# зададим толщину ребер графа \n",
    "n_nodes = G.number_of_nodes()\n",
    "all_weights = []\n",
    "for (node1,node2,data) in G.edges(data=True):\n",
    "        all_weights.append(data['weight']) \n",
    "unique_weights = list(set(all_weights))\n",
    "\n",
    "for weight in unique_weights:\n",
    "        weighted_edges = [(node1,node2) for (node1,node2,edge_attr) in G.edges(data=True) if edge_attr['weight']==weight]\n",
    "        width = np.log(weight)*n_nodes*10/sum(all_weights)\n",
    "        nx.draw_networkx_edges(G,pos,edgelist=weighted_edges,width=width, arrowstyle='->')\n",
    "\n",
    "plt.title('Морские порты Российской Федерации')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Временные ряды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(engine) as session:\n",
    "        query = session.query(Port.name, Port.basin, Port.lat, Port.lon).\\\n",
    "                        order_by(Port.id)\n",
    "        port_df = pd.read_sql(query.statement, query.session.bind)\n",
    "\n",
    "def arrival_dynamics_with_tonnage(ports, ship_type, date_from, date_to):\n",
    "        with Session(engine) as session:\n",
    "                query = session.query(\n",
    "                                func.date_trunc('day', PortCall.arrival).label('arrival_day'), \n",
    "                                func.sum(Ship.tonnage).label('tonnage')).\\\n",
    "                                join(PortCall, PortCall.ship_id == Ship.id).\\\n",
    "                                join(Port, PortCall.port_id == Port.id).\\\n",
    "                                filter(PortCall.arrival >= date_from).\\\n",
    "                                filter(PortCall.arrival <= date_to).\\\n",
    "                                filter(Port.name.in_(tuple(ports))).\\\n",
    "                                filter(Ship.type == ship_type).\\\n",
    "                                filter(PortCall.arrival != None).\\\n",
    "                                filter(PortCall.departure != None).\\\n",
    "                                filter(Ship.tonnage != np.NaN).\\\n",
    "                                group_by('arrival_day').\\\n",
    "                                order_by('arrival_day')\n",
    "                \n",
    "                df = pd.read_sql(query.statement, query.session.bind)\n",
    "\n",
    "        days = set(df['arrival_day'].to_list())\n",
    "        datelist = set(pd.date_range(min(days), max(days)).tolist())\n",
    "        for d in (datelist-days):\n",
    "                df.loc[len(df.index)] = [d, 0] \n",
    "        \n",
    "        return df.sort_values('arrival_day').set_index('arrival_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports = port_df.loc[port_df['Бассейн'] == 'Балтийский бассейн', 'Название порта'].to_list()\n",
    "with Session(engine) as session:\n",
    "    query = session.query(\n",
    "                    func.sum(Ship.tonnage).label('tonnage'),\n",
    "                    PortCall.port_call).\\\n",
    "                    join(PortCall, PortCall.ship_id == Ship.id).\\\n",
    "                    join(Port, PortCall.port_id == Port.id).\\\n",
    "                    filter(Port.name.in_(tuple(ports))).\\\n",
    "                    filter(Ship.tonnage != np.NaN).\\\n",
    "                    group_by(PortCall.port_call).\\\n",
    "                    order_by('tonnage')\n",
    "    port_tonnage = pd.read_sql(query.statement, query.session.bind)\n",
    "\n",
    "port_tonnage[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports = port_df.loc[port_df['Бассейн'] == 'Азово-Черноморский бассейн', 'Название порта'].to_list()\n",
    "with Session(engine) as session:\n",
    "    query = session.query(\n",
    "                    func.count(PortCall.ship_id).label('count'),\n",
    "                    Ship.type).\\\n",
    "                    join(PortCall, PortCall.ship_id == Ship.id).\\\n",
    "                    join(Port, PortCall.port_id == Port.id).\\\n",
    "                    filter(Port.name.in_(tuple(ports))).\\\n",
    "                    filter(Ship.tonnage != np.NaN).\\\n",
    "                    group_by(Ship.type).\\\n",
    "                    order_by('count')\n",
    "    ship_type_count = pd.read_sql(query.statement, query.session.bind)\n",
    "\n",
    "ship_type_count[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_slice = {\n",
    "    1: {'ports': port_df.loc[port_df['Бассейн'] == 'Азово-Черноморский бассейн', 'Название порта'].to_list(),\n",
    "        'ship_type': '60 / General cargo/multi-purpose ship',\n",
    "        'name': 'Азово-Черноморский бассейн'},\n",
    "    2: {'ports': port_df.loc[port_df['Бассейн'] == 'Дальневосточный бассейн', 'Название порта'].to_list(),\n",
    "        'ship_type': '13 / Oil tanker',\n",
    "        'name': 'Дальневосточный бассейн'},\n",
    "    3: {'ports': port_df.loc[port_df['Бассейн'] == 'Балтийский бассейн', 'Название порта'].to_list(),\n",
    "        'ship_type': '60 / General cargo/multi-purpose ship',\n",
    "        'name': 'Балтийский бассейн'},\n",
    "    4: {'ports': ['Новороссийск'],\n",
    "        'ship_type': '13 / Oil tanker',\n",
    "        'name': 'Новороссийск'},\n",
    "    5: {'ports': ['Восточный'],\n",
    "        'ship_type': '13 / Oil tanker',\n",
    "        'name': 'Восточный'},\n",
    "    6: {'ports': ['Санкт-Петербург'],\n",
    "        'ship_type': '60 / General cargo/multi-purpose ship',\n",
    "        'name': 'Санкт-Петербург'}\n",
    "}\n",
    "\n",
    "for i_data_slice in range(1, 7):\n",
    "    date_from=date(2015, 1, 1) \n",
    "    date_to=date(2023, 12, 31)\n",
    "    ports = data_slice[i_data_slice]['ports']\n",
    "    ship_type = data_slice[i_data_slice]['ship_type']\n",
    "    df = arrival_dynamics_with_tonnage(ports, ship_type, date_from, date_to)\n",
    "    df_min, df_max = df.min(), df.max()\n",
    "    df = (df-df_min)/(df_max -df_min)\n",
    "    df = df.resample('D').sum()\n",
    "    train_start = date(2020, 12, 31)\n",
    "    data_slice[i_data_slice]['train'] = df.loc[:train_start]\n",
    "    data_slice[i_data_slice]['test']  = df.loc[train_start:]\n",
    "    data_slice[i_data_slice]['all_data']  = df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходные временные ряды (рис 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=2, \n",
    "                    subplot_titles = [data_slice[(i % 2) * 3 + i // 2 + 1]['name'] for i in range(6)],\n",
    "                    vertical_spacing = 0.1, horizontal_spacing=0.05)\n",
    "for i in range(6):\n",
    "    i_data_slice = i + 1\n",
    "    row = i % 3 + 1\n",
    "    col = i // 3 + 1\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_slice[i_data_slice]['train'].index, y=data_slice[i_data_slice]['train'].tonnage, \n",
    "        name='Train', line=dict(color='MediumPurple'), showlegend=False), row=row, col=col)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_slice[i_data_slice]['test'].index, y=data_slice[i_data_slice]['test'].tonnage, \n",
    "        name='Test', line=dict(color='Olive'), showlegend=False), row=row, col=col)\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=50, b=20))\n",
    "fig.update_annotations(font_size=12)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения периода методом периодограмм (таблица 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def period_calculation(data):\n",
    "\n",
    "    # Вычисление периодограммы\n",
    "    frequencies, power_spectrum = periodogram(data.tonnage)\n",
    "\n",
    "    # Нахождение наибольшей мощности\n",
    "    max_power_idx = np.argmax(power_spectrum)\n",
    "    dominant_frequency = frequencies[max_power_idx]\n",
    "\n",
    "    # Определение периода\n",
    "    period = 1 / dominant_frequency\n",
    "    return period\n",
    "\n",
    "for i in range(1, 7):\n",
    "    print(data_slice[i][\"name\"],':', period_calculation(data_slice[i][\"train\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разложение временного ряда на составляюще методом STL (рис 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_STL(data_slice, period=365, threshold=3, forecast_period=365, resample = 'D', stl_train=True, arima_args=dict(order=(1, 1, 0), trend=\"t\")):\n",
    "    model = dict()\n",
    "    for i in range(1, 7):\n",
    "        model[i] = dict()\n",
    "        if stl_train:\n",
    "            data = data_slice[i]['train'].tonnage.resample(resample).mean()\n",
    "        else:\n",
    "             data = data_slice[i]['all_data'].tonnage.resample(resample).mean()\n",
    "        stl = STL(data, period=period)\n",
    "        res = stl.fit()\n",
    "\n",
    "        model[i]['trend'] = res.trend\n",
    "        model[i]['seasonal'] = res.seasonal\n",
    "        model[i]['residual'] = res.resid\n",
    "\n",
    "        # Выявление аномалий\n",
    "        model[i]['threshold'] = threshold * np.std(res.resid)\n",
    "\n",
    "        #прогноз\n",
    "        stlf = STLForecast(data, ARIMA, model_kwargs=arima_args)\n",
    "        stlf_res = stlf.fit()\n",
    "\n",
    "        model[i]['forecast'] = stlf_res.forecast(forecast_period)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstl = model_STL(data_slice, period=52, threshold=3,forecast_period=130,  resample='W') # 52, 130, 'W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=2, \n",
    "                    subplot_titles = [data_slice[(i % 2) * 3 + i // 2 + 1]['name'] for i in range(6)],\n",
    "                    vertical_spacing = 0.1, horizontal_spacing=0.05)\n",
    "for i in range(6):\n",
    "    i_data_slice = i + 1\n",
    "    row = i % 3 + 1\n",
    "    col = i // 3 + 1\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_slice[i_data_slice]['train'].index, y=data_slice[i_data_slice]['train'].tonnage, \n",
    "        name='Train', line=dict(color='MediumPurple'), showlegend=False), row=row, col=col)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=mstl[i_data_slice]['trend'].index, y=mstl[i_data_slice]['trend'], \n",
    "        name='Train', line=dict(color='navy'), showlegend=False), row=row, col=col)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=mstl[i_data_slice]['seasonal'].index, y=mstl[i_data_slice]['seasonal'], \n",
    "        name='Train', line=dict(color='navy'), showlegend=False), row=row, col=col)\n",
    "    \n",
    "fig.update_layout(margin=dict(l=20, r=20, t=50, b=20))\n",
    "fig.update_annotations(font_size=12)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные для описания тренда (Таблица 7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 7):\n",
    "    print(mstl[i]['trend'].describe())\n",
    "    print(mstl[i]['trend'].max() - mstl[i]['trend'].min())\n",
    "    print(mstl[i]['trend'][-1] - mstl[i]['trend'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогноз с помощью STLForecast (рис 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_STL(data_slice, model_stl, train=True, test=True):\n",
    "    fig = make_subplots(rows=3, cols=2, \n",
    "                        subplot_titles = [data_slice[(i % 2) * 3 + i // 2 + 1]['name'] for i in range(6)],\n",
    "                        vertical_spacing = 0.1, horizontal_spacing=0.05)\n",
    "    for i in range(6):\n",
    "        i_data_slice = i + 1\n",
    "        row = i % 3 + 1\n",
    "        col = i // 3 + 1\n",
    "        if train:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=data_slice[i_data_slice]['train'].index, y=data_slice[i_data_slice]['train'].tonnage, \n",
    "                name='Train', line=dict(color='MediumPurple'), showlegend=False), row=row, col=col)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=model_stl[i_data_slice]['trend'].index, y=model_stl[i_data_slice]['trend']+model_stl[i_data_slice]['seasonal'], \n",
    "                name='Train', line=dict(color='navy'), showlegend=False), row=row, col=col)\n",
    "        if test:\n",
    "            y = data_slice[i_data_slice]['test'].tonnage.resample('W').mean()\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=y.index, y=y, \n",
    "                name='Test', line=dict(color='Olive'), showlegend=False), row=row, col=col)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=model_stl[i_data_slice]['forecast'][:pd.Timestamp('2023-04-02')].index, y=model_stl[i_data_slice]['forecast'][:pd.Timestamp('2023-04-02')], \n",
    "                name='Train', line=dict(color='mediumvioletred'), showlegend=False), row=row, col=col)\n",
    "    fig.update_layout(margin=dict(l=20, r=20, t=50, b=20))\n",
    "    fig.update_annotations(font_size=12)\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "plot_STL(data_slice, mstl, train=False, test=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогноз с помощью LSTM (рис 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, lookback):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=lookback, hidden_size=64, num_layers=1, batch_first=True).type(torch.float64)\n",
    "        self.linear = nn.Linear(64, lookback).type(torch.float64)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "def create_dataset(dataset, lookback):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-2*lookback):\n",
    "        feature = dataset[i:i+lookback]\n",
    "        target = dataset[i+lookback+1:i+2*lookback+1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm(X_train, y_train, X_test, y_test, lookback, n_epochs = 1000, print_bool=True):\n",
    "    model = LSTM(lookback)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loader = DataLoader(TensorDataset(X_train, y_train), shuffle=True, batch_size=8)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.type(torch.float64)\n",
    "            y_batch = y_batch.type(torch.float64)\n",
    "            y_pred = model(X_batch)\n",
    "            # print(y_pred.dtype)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Validation\n",
    "        if (epoch % 100 != 0) and print_bool:\n",
    "            continue\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_train)\n",
    "            train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
    "            y_pred = model(X_test)\n",
    "            test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
    "        print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict()\n",
    "for j in range(1, 7):\n",
    "    print()\n",
    "    print(data_slice[j]['name'])\n",
    "    res[j] = dict()\n",
    "    res[j]['name'] = data_slice[j]['name']\n",
    "    \n",
    "    #условия\n",
    "    date_from=date(2015, 1, 1) \n",
    "    date_to=date(2023, 12, 31)\n",
    "    ports = data_slice[j]['ports']\n",
    "    ship_type = data_slice[j]['ship_type']\n",
    "\n",
    "    #изначальный датасет\n",
    "    df = arrival_dynamics_with_tonnage(ports, ship_type, date_from, date_to).tonnage\n",
    "    df_min, df_max = df.min(), df.max()\n",
    "    df = (df-df_min)/(df_max -df_min)\n",
    "    df = df.resample('W').mean()\n",
    "\n",
    "    # датасет разбитый на train и test\n",
    "    lookback = 52\n",
    "    train_border = date(2020, 12, 31)\n",
    "    train, test = df.loc[:train_border], df.loc[train_border - timedelta(lookback*7):]\n",
    "    X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "    X_test, y_test = create_dataset(test, lookback=lookback)\n",
    "\n",
    "    # обучение\n",
    "    model = model_lstm(X_train, y_train, X_test, y_test, lookback, n_epochs = 300, print_bool=True)\n",
    "\n",
    "    # прогноз\n",
    "    forecast_df = pd.Series([0]*len(test), index=test.index)\n",
    "    input = X_test[0]\n",
    "    forecast_df[:lookback] = pd.Series(X_test[0], index=test[:lookback].index)\n",
    "    without_pred = len(forecast_df) - lookback\n",
    "\n",
    "    for i in range(len(forecast_df) - lookback):\n",
    "        input = forecast_df[i:i+lookback]\n",
    "        input = torch.tensor(input).unsqueeze(0).type(torch.float64)\n",
    "        \n",
    "        if without_pred >= lookback:\n",
    "            forecast_df[i+lookback: i+2*lookback] = pd.Series(model(input)[0].detach().numpy(), index=test[i+lookback: i+2*lookback].index)\n",
    "        else:\n",
    "            pred = model(input)[0].detach().numpy()[:without_pred]\n",
    "            forecast_df[i+lookback: i+lookback+without_pred] = pd.Series(pred, index=test[i+lookback: i+lookback+without_pred].index)\n",
    "        without_pred -=1\n",
    "\n",
    "    forecast_df = forecast_df.loc[train_border:]\n",
    "    test = test[train_border:]\n",
    "    res[j]['forecast'] = forecast_df\n",
    "    res[j]['test'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=2, \n",
    "                    subplot_titles = [data_slice[(i % 2) * 3 + i // 2 + 1]['name'] for i in range(6)],\n",
    "                    vertical_spacing = 0.1, horizontal_spacing=0.05)\n",
    "for i in range(6):\n",
    "    fig.add_trace(go.Scatter(x=res[i + 1]['forecast'].index, y=res[i + 1]['forecast'], name='prediction',\n",
    "                             line=dict(color='mediumvioletred'), showlegend=False), row=i%3+1, col=i//3+1)\n",
    "    fig.add_trace(go.Scatter(x=res[i + 1]['test'].index, y=res[i + 1]['test'], name='ground truth',\n",
    "                             line=dict(color='Olive'), showlegend=False), row=i%3+1, col=i//3+1)\n",
    "    \n",
    "fig.update_layout(margin=dict(l=20, r=20, t=50, b=20))\n",
    "fig.update_annotations(font_size=12)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-критерий (рис 22 и 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.read_excel('Приложения/external_events_15_22.xlsx', parse_dates=True)\n",
    "events_df['Average_date'] = events_df.apply(lambda x: x['Дата начала'] + (x['Дата начала'] - x['Дата окончания'])/2, axis=1)\n",
    "\n",
    "events_df.loc[(events_df['География влияния'] =='Порт') | (events_df['География влияния'] =='Бассейн'), 'География влияния'] = events_df['Подробнее']\n",
    "events_df = events_df.drop(columns='Подробнее')\n",
    "\n",
    "influence_rows = []\n",
    "\n",
    "for basin in port_df['Бассейн'].drop_duplicates():\n",
    "    influence_rows.append([basin, basin, 'bb'])\n",
    "    influence_rows.append([basin, 'Россия', 'brf'])\n",
    "for port in port_df['Название порта']:\n",
    "    influence_rows.append([port, port, 'p'])\n",
    "    influence_rows.append([port, 'Россия', 'prf'])  \n",
    "    basin = port_df.loc[port_df['Название порта'] == port, 'Бассейн'].to_list()[0]\n",
    "    influence_rows.append([port, basin, 'pb'])  \n",
    "    influence_rows.append([basin,port, 'bp'])\n",
    "    influence_rows += [[port, p, 'pbp'] for p in port_df.loc[port_df['Бассейн'] == basin, 'Название порта'].to_list() if port !=p] \n",
    "\n",
    "dependency_df = pd.DataFrame(influence_rows, columns=['Influenced', 'Influencer', 'Influence_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_df = events_df.copy()\n",
    "l = 1 # рассматриваемая локация\n",
    "residual = STL(data_slice[l]['all_data'].tonnage, period=365).fit().resid\n",
    "periods = [7, 30, 30*3, 30*6, 365]\n",
    "for p in periods:\n",
    "    significance_df[f'h={p}, before mean'] = np.NaN\n",
    "    significance_df[f'h={p}, before std'] = np.NaN\n",
    "    significance_df[f'h={p}, after mean'] = np.NaN\n",
    "    significance_df[f'h={p}, after std'] = np.NaN\n",
    "    significance_df[f'h={p}, t-statistic'] = np.NaN\n",
    "    significance_df[f'h={p}, p-value'] = np.NaN\n",
    "\n",
    "Influenced = data_slice[l]['name']\n",
    "Influencer = dependency_df.loc[dependency_df['Influenced'] == Influenced, 'Influencer'].to_list()\n",
    "ev = events_df.loc[events_df['География влияния'].isin(Influencer)]\n",
    "\n",
    "for i, row in ev.iterrows():\n",
    "    event_date = row['Average_date']\n",
    "    for p in periods:\n",
    "        before_event = residual.loc[(residual.index < pd.to_datetime(event_date)) & (residual.index >= event_date-timedelta(days=p))]\n",
    "        after_event = residual.loc[(residual.index >= pd.to_datetime(event_date)) & (residual.index <= event_date+timedelta(days=p))]\n",
    "\n",
    "        t_stat, p_value = ttest_ind(before_event, after_event)\n",
    "\n",
    "        significance_df.at[i, f'h={p}, before mean'] = before_event.mean()\n",
    "        significance_df.at[i, f'h={p}, before std'] =  after_event.mean()\n",
    "        significance_df.at[i, f'h={p}, after mean'] = before_event.std()\n",
    "        significance_df.at[i, f'h={p}, after std'] = after_event.std()\n",
    "        significance_df.at[i, f'h={p}, t-statistic'] = t_stat\n",
    "        significance_df.at[i, f'h={p}, p-value'] = p_value\n",
    "\n",
    "significance_df.loc[(abs(significance_df['h=30, t-statistic'])>=1.96) & (significance_df['h=30, p-value'] < 0.05), ['Average_date', 'География влияния', 'Описание', 'Влияние', 'h=30, t-statistic', 'h=30, p-value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_df = significance_df.copy()\n",
    "h_df['max_d'] = 0\n",
    "h_df['diff'] = np.NaN\n",
    "for i, h in enumerate(periods):\n",
    "    h_df[h] = h_df.apply(lambda x: x[f'h={h}, before mean']-x[f'h={h}, after mean'] if abs(x[f'h={h}, t-statistic'])>1.96 and x[f'h={h}, p-value']<0.05 else np.NaN, axis=1)\n",
    "    h_df['max_d'] = h_df.apply(lambda x: h if not pd.isna(x[h]) else x['max_d'], axis=1)\n",
    "    h_df['diff'] = h_df.apply(lambda x: x[h] if not pd.isna(x[h]) else x['diff'], axis=1)\n",
    "h_df = h_df.dropna(subset='diff')\n",
    "h_df['Влияние'] = h_df['Влияние'].astype('int').astype('str')\n",
    "\n",
    "pt = pd.pivot_table(h_df, index='Влияние', values=['diff', 'max_d'], aggfunc=['mean', 'std', 'min', 'max'], dropna=False, fill_value=0)\n",
    "pt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рисунок представляет собой шум от временного ряда для Азово-Черноморского бассейна после разложения STL c наложением значимых событий (рис 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=residual.index, y=residual))\n",
    "h_df = h_df.set_index('Average_date')\n",
    "for i, r in h_df.loc[(h_df['h=30, p-value'] < 0.05)].iterrows():\n",
    "    fig.add_shape(\n",
    "                type='line',\n",
    "                line_color='gold',\n",
    "                line_width=3,\n",
    "                opacity=.5,\n",
    "                x0=i,\n",
    "                x1=i,\n",
    "                xref='x',\n",
    "                y0=residual.min()*1.1,\n",
    "                y1=residual.max()*1.1,\n",
    "                yref='y',\n",
    "                )\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделирование временного ряда с помощью STL и добавлением экзогенного параметра (рис 25)\n",
    "Также подсчитаны MAE, MSE, MAPE для STL и STLexog (таблицы 8-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = dict()\n",
    "arima_args = {1: dict(order=(1, 1, 0), trend=\"t\"), 2: dict(order=(1, 0, 0)), 3: dict(order=(1, 1, 0)),\n",
    "               4: dict(order=(1, 1, 0)), 5: dict(order=(1, 1, 0)), 6:dict(order=(1, 1, 1))}\n",
    "coeff = {1: 2, 2: .5, 3: 1, 4: 1, 5: .25, 6: 1}\n",
    "\n",
    "for l in range(1, 7): # рассматриваемая локация\n",
    "    print(data_slice[l]['name'])\n",
    "    significance_df = events_df.copy()\n",
    "    residual = STL(data_slice[l]['all_data'].tonnage, period=365).fit().resid\n",
    "    periods = [7, 30, 30*3, 30*6, 365]\n",
    "    for p in periods:\n",
    "        significance_df[f'h={p}, before mean'] = np.NaN\n",
    "        significance_df[f'h={p}, before std'] = np.NaN\n",
    "        significance_df[f'h={p}, after mean'] = np.NaN\n",
    "        significance_df[f'h={p}, after std'] = np.NaN\n",
    "        significance_df[f'h={p}, t-statistic'] = np.NaN\n",
    "        significance_df[f'h={p}, p-value'] = np.NaN\n",
    "\n",
    "    Influenced = data_slice[l]['name']\n",
    "    Influencer = dependency_df.loc[dependency_df['Influenced'] == Influenced, 'Influencer'].to_list()\n",
    "    ev = events_df.loc[events_df['География влияния'].isin(Influencer)]\n",
    "\n",
    "    for i, row in ev.iterrows():\n",
    "        event_date = row['Average_date']\n",
    "        for p in periods:\n",
    "            before_event = residual.loc[(residual.index < pd.to_datetime(event_date)) & (residual.index >= event_date-timedelta(days=p))]\n",
    "            after_event = residual.loc[(residual.index >= pd.to_datetime(event_date)) & (residual.index <= event_date+timedelta(days=p))]\n",
    "\n",
    "            t_stat, p_value = ttest_ind(before_event, after_event)\n",
    "\n",
    "            significance_df.at[i, f'h={p}, before mean'] = before_event.mean()\n",
    "            significance_df.at[i, f'h={p}, before std'] =  after_event.mean()\n",
    "            significance_df.at[i, f'h={p}, after mean'] = before_event.std()\n",
    "            significance_df.at[i, f'h={p}, after std'] = after_event.std()\n",
    "            significance_df.at[i, f'h={p}, t-statistic'] = t_stat\n",
    "            significance_df.at[i, f'h={p}, p-value'] = p_value\n",
    "\n",
    "    h_df = significance_df.copy()\n",
    "    h_df['max_d'] = 0\n",
    "    h_df['diff'] = np.NaN\n",
    "    for i, h in enumerate(periods):\n",
    "        h_df[h] = h_df.apply(lambda x: x[f'h={h}, before mean']-x[f'h={h}, after mean'] if abs(x[f'h={h}, t-statistic'])>1.96 and x[f'h={h}, p-value']<0.05 else np.NaN, axis=1)\n",
    "        h_df['max_d'] = h_df.apply(lambda x: h if not pd.isna(x[h]) else x['max_d'], axis=1)\n",
    "        h_df['diff'] = h_df.apply(lambda x: x[h] if not pd.isna(x[h]) else x['diff'], axis=1)\n",
    "    h_df = h_df.dropna(subset='diff')\n",
    "    h_df['Влияние'] = h_df['Влияние'].astype('int').astype('str')\n",
    "\n",
    "    pt = pd.pivot_table(h_df, index='Влияние', values=['diff', 'max_d'], aggfunc=['mean', 'std', 'min', 'max'], dropna=False, fill_value=0)\n",
    "    #print(pt)\n",
    "\n",
    "    stlf = STLForecast(data_slice[l]['train'].tonnage.resample('W').mean(), ARIMA, model_kwargs=arima_args[l]).fit()\n",
    "    test_prediction =stlf.forecast(117)\n",
    "    test_data = data_slice[l]['test'].tonnage.resample('W').mean()[1:]\n",
    "\n",
    "    a = h_df.copy().set_index('Average_date')\n",
    "    a['Влияние'] = a['Влияние'].astype('int').astype('str')\n",
    "\n",
    "    biased_correction = list()\n",
    "    n = 500\n",
    "    d_max = test_prediction.index.max()\n",
    "\n",
    "    for i in range(n):\n",
    "        biased_correction_i = test_prediction.copy()\n",
    "        for j, r in a.iterrows():\n",
    "            \n",
    "            diff = np.random.normal(pt.at[r['Влияние'], ('mean', 'diff')], pt.at[r['Влияние'], ('std', 'diff')])\n",
    "            t = np.random.normal(pt.at[r['Влияние'], ('mean', 'max_d')], pt.at[r['Влияние'], ('std', 'max_d')])\n",
    "\n",
    "            t = min(j+timedelta(max(t, 0)), d_max)\n",
    "            biased_correction_i[j:t] = diff * coeff[l]\n",
    "        biased_correction_i +=test_prediction\n",
    "        biased_correction.append(biased_correction_i)\n",
    "\n",
    "    bc = test_prediction.copy()\n",
    "    for i, d in enumerate(test_prediction.index):\n",
    "        bc[d] = 0\n",
    "        for j in range(n):\n",
    "            bc[d] += biased_correction[j][d]\n",
    "        bc[d] /= n\n",
    "\n",
    "    mae_biased = mean_absolute_error(test_data, bc)\n",
    "    mae_test = mean_absolute_error(test_data, test_prediction)\n",
    "    print(\"MAE для прогноза на основе STL: \", mae_test)\n",
    "    print(\"MAE для прогноза на основе метода смещенной корректировки: \", mae_biased)\n",
    "    mse_biased = mean_squared_error(test_data, bc)\n",
    "    mse_test = mean_squared_error(test_data, test_prediction)\n",
    "    print(\"MSE для прогноза на основе STL: \", mse_test)\n",
    "    print(\"MSE для прогноза на основе метода смещенной корректировки: \", mse_biased)\n",
    "    mape_biased = mean_absolute_percentage_error(test_data, bc)\n",
    "    mape_test = mean_absolute_percentage_error(test_data, test_prediction)\n",
    "    print(\"MAPE для прогноза на основе STL: \", mape_test)\n",
    "    print(\"MAPE для прогноза на основе метода смещенной корректировки: \", mape_biased)\n",
    "\n",
    "\n",
    "\n",
    "    traces[l] = dict()\n",
    "    traces[l]['Prediction'] = go.Scatter(x=test_prediction.index, y=test_prediction, name='Prediction', showlegend=False, line=dict(color='mediumvioletred'))\n",
    "    traces[l]['Biased correction'] = go.Scatter(x=bc.index, y=bc, name='Biased correction', showlegend=False, line=dict(color='blue'))\n",
    "    traces[l]['Ground truth'] = go.Scatter(x=test_data.index, y=test_data, name='Ground truth', showlegend=False, line=dict(color='Olive'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=2, \n",
    "                    subplot_titles = [data_slice[(i % 2) * 3 + i // 2 + 1]['name'] for i in range(6)],\n",
    "                    vertical_spacing = 0.1, horizontal_spacing=0.05)\n",
    "for i in range(6):\n",
    "    fig.add_trace(traces[i+1]['Ground truth'], row=i%3+1, col=i//3+1)\n",
    "    fig.add_trace(traces[i+1]['Prediction'], row=i%3+1, col=i//3+1)\n",
    "    fig.add_trace(traces[i+1]['Biased correction'], row=i%3+1, col=i//3+1)\n",
    "    \n",
    "fig.update_layout(margin=dict(l=20, r=20, t=50, b=20))\n",
    "fig.update_annotations(font_size=12)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделирование временного ряда с помощью LSTM и добавлением экзогенного параметра (рис 26)\n",
    "Также подсчитаны MAE, MSE, MAPE для LSTM и LSTMexog (таблицы 8-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "for j in range(1, 7):\n",
    "    print()\n",
    "    print(data_slice[j]['name'])\n",
    "    \n",
    "    #условия\n",
    "    date_from=date(2015, 1, 1) \n",
    "    date_to=date(2023, 12, 31)\n",
    "    ports = data_slice[j]['ports']\n",
    "    ship_type = data_slice[j]['ship_type']\n",
    "\n",
    "    #изначальный датасет\n",
    "    df = arrival_dynamics_with_tonnage(ports, ship_type, date_from, date_to).tonnage\n",
    "    df_min, df_max = df.min(), df.max()\n",
    "    df = (df-df_min)/(df_max -df_min)\n",
    "    df = df.resample('W').mean()\n",
    "\n",
    "    # датасет разбитый на train и test\n",
    "    lookback = 52\n",
    "    train_border = date(2020, 12, 31)\n",
    "    train, test = df.loc[:train_border], df.loc[train_border - timedelta(lookback*7):]\n",
    "    X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "    X_test, y_test = create_dataset(test, lookback=lookback)\n",
    "\n",
    "    # обучение\n",
    "    models[j] = dict()\n",
    "    models[j]['model'] = model_lstm(X_train, y_train, X_test, y_test, lookback, n_epochs = 500, print_bool=True)\n",
    "    models[j]['train'] = train\n",
    "    models[j]['test'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_args = {1: dict(order=(1, 1, 0), trend=\"t\"), 2: dict(order=(1, 0, 0)), 3: dict(order=(1, 1, 0)),\n",
    "               4: dict(order=(1, 1, 0)), 5: dict(order=(1, 1, 0)), 6:dict(order=(1, 1, 1))}\n",
    "coeff = {1: 2, 2: .5, 3: 1, 4: 1, 5: .25, 6: 1}\n",
    "pt = dict()\n",
    "seg = dict()\n",
    "\n",
    "for l in range(1, 7): # рассматриваемая локация\n",
    "    significance_df = events_df.copy()\n",
    "    residual = STL(data_slice[l]['all_data'].tonnage, period=365).fit().resid\n",
    "    periods = [7, 30, 30*3, 30*6, 365]\n",
    "    for p in periods:\n",
    "        significance_df[f'h={p}, before mean'] = np.NaN\n",
    "        significance_df[f'h={p}, before std'] = np.NaN\n",
    "        significance_df[f'h={p}, after mean'] = np.NaN\n",
    "        significance_df[f'h={p}, after std'] = np.NaN\n",
    "        significance_df[f'h={p}, t-statistic'] = np.NaN\n",
    "        significance_df[f'h={p}, p-value'] = np.NaN\n",
    "\n",
    "    Influenced = data_slice[l]['name']\n",
    "    Influencer = dependency_df.loc[dependency_df['Influenced'] == Influenced, 'Influencer'].to_list()\n",
    "    ev = events_df.loc[events_df['География влияния'].isin(Influencer)]\n",
    "\n",
    "    for i, row in ev.iterrows():\n",
    "        event_date = row['Average_date']\n",
    "        for p in periods:\n",
    "            before_event = residual.loc[(residual.index < pd.to_datetime(event_date)) & (residual.index >= event_date-timedelta(days=p))]\n",
    "            after_event = residual.loc[(residual.index >= pd.to_datetime(event_date)) & (residual.index <= event_date+timedelta(days=p))]\n",
    "\n",
    "            t_stat, p_value = ttest_ind(before_event, after_event)\n",
    "\n",
    "            significance_df.at[i, f'h={p}, before mean'] = before_event.mean()\n",
    "            significance_df.at[i, f'h={p}, before std'] =  after_event.mean()\n",
    "            significance_df.at[i, f'h={p}, after mean'] = before_event.std()\n",
    "            significance_df.at[i, f'h={p}, after std'] = after_event.std()\n",
    "            significance_df.at[i, f'h={p}, t-statistic'] = t_stat\n",
    "            significance_df.at[i, f'h={p}, p-value'] = p_value\n",
    "\n",
    "    h_df = significance_df.copy()\n",
    "    h_df['max_d'] = 0\n",
    "    h_df['diff'] = np.NaN\n",
    "    for i, h in enumerate(periods):\n",
    "        h_df[h] = h_df.apply(lambda x: x[f'h={h}, before mean']-x[f'h={h}, after mean'] if abs(x[f'h={h}, t-statistic'])>1.96 and x[f'h={h}, p-value']<0.05 else np.NaN, axis=1)\n",
    "        h_df['max_d'] = h_df.apply(lambda x: h if not pd.isna(x[h]) else x['max_d'], axis=1)\n",
    "        h_df['diff'] = h_df.apply(lambda x: x[h] if not pd.isna(x[h]) else x['diff'], axis=1)\n",
    "    h_df = h_df.dropna(subset='diff')\n",
    "    h_df['Влияние'] = h_df['Влияние'].astype('int').astype('str')\n",
    "\n",
    "    seg[l] = h_df.set_index('Average_date')\n",
    "    pt[l] = pd.pivot_table(h_df, index='Влияние', values=['diff', 'max_d'], aggfunc=['mean', 'std', 'min', 'max'], dropna=False, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict()\n",
    "steps = {1: 52, 2: 52, 3: 13, 4: 2, 5: 2, 6: 26}\n",
    "coeff = {1: 1/64, 2: 1/16, 3: 1/4, 4: 1/4, 5: 1/4, 6: 1/8}\n",
    "for j in range(1, 7):\n",
    "    print()\n",
    "    print(data_slice[j]['name'])\n",
    "\n",
    "    # обучение\n",
    "    model = models[j]['model']\n",
    "    test  = models[j]['test']\n",
    "    train = models[j]['train']\n",
    "\n",
    "    n = 100\n",
    "    forecast_df_list = list()\n",
    "    for k in tqdm(range(n)):\n",
    "        # прогноз\n",
    "        forecast_df = pd.Series([0]*len(test), index=test.index)\n",
    "        input = X_test[0]\n",
    "        forecast_df[:lookback] = pd.Series(X_test[0], index=test[:lookback].index)\n",
    "        without_pred = len(forecast_df) - lookback\n",
    "\n",
    "\n",
    "        # прогноз\n",
    "        forecast_df = pd.Series([0]*len(test), index=test.index)\n",
    "        input = X_test[0]\n",
    "        forecast_df[:lookback] = pd.Series(X_test[0], index=test[:lookback].index)\n",
    "        \n",
    "        without_pred = len(forecast_df) - lookback\n",
    "        step = steps[j]\n",
    "        \n",
    "        for i in range(0, len(forecast_df) - lookback, step):\n",
    "            input = forecast_df[i:i+lookback]\n",
    "            input = torch.tensor(input).unsqueeze(0).type(torch.float64)\n",
    "\n",
    "            \n",
    "            \n",
    "            if without_pred >= lookback:\n",
    "                forecast_df[i+lookback: i+2*lookback] = pd.Series(model(input)[0].detach().numpy(), index=test[i+lookback: i+2*lookback].index)\n",
    "                ind = forecast_df[i+lookback: i+2*lookback].index\n",
    "                for k, r in seg[j].loc[(seg[j].index >= ind.min()) & (seg[j].index <= ind.max())].iterrows():\n",
    "                \n",
    "                    diff = np.random.normal(pt[l].at[r['Влияние'], ('mean', 'diff')], pt[l].at[r['Влияние'], ('std', 'diff')])\n",
    "                    t = np.random.normal(pt[l].at[r['Влияние'], ('mean', 'max_d')], pt[l].at[r['Влияние'], ('std', 'max_d')])\n",
    "                    t = min(k+timedelta(max(t, 0)), d_max)\n",
    "                    forecast_df[k:t] += diff * coeff[j]\n",
    "            else:\n",
    "                pred = model(input)[0].detach().numpy()[:without_pred]\n",
    "                forecast_df[i+lookback: i+lookback+without_pred] = pd.Series(pred, index=test[i+lookback: i+lookback+without_pred].index)\n",
    "                ind = forecast_df[i+lookback: i+lookback+without_pred].index\n",
    "                for k, r in seg[j].loc[(seg[j].index >= ind.min()) & (seg[j].index <= ind.max())].iterrows():\n",
    "                \n",
    "                    diff = np.random.normal(pt[l].at[r['Влияние'], ('mean', 'diff')], pt[l].at[r['Влияние'], ('std', 'diff')])\n",
    "                    t = np.random.normal(pt[l].at[r['Влияние'], ('mean', 'max_d')], pt[l].at[r['Влияние'], ('std', 'max_d')])\n",
    "                    t = min(k+timedelta(max(t, 0)), d_max)\n",
    "                    forecast_df[k:t] += diff * coeff[j]\n",
    "            without_pred -=step\n",
    "        forecast_df = forecast_df.loc[train_border:]\n",
    "        forecast_df_list.append(forecast_df.copy())\n",
    "\n",
    "    for d in forecast_df_list[-1].index:\n",
    "        forecast_df[d] = 0\n",
    "        for i in range(n):\n",
    "            forecast_df[d] += forecast_df_list[i][d]\n",
    "        forecast_df[d] /= n\n",
    "    test = test[train_border:]\n",
    "    #print(forecast_df)\n",
    "    res[j] = dict()\n",
    "    res[j]['forecast'] = forecast_df\n",
    "    res[j]['test'] = test\n",
    "\n",
    "    mae_lstm = mean_absolute_error(res[j]['test'], res[j]['forecast'])\n",
    "    mse_lstm = mean_squared_error(res[j]['test'], res[j]['forecast'])\n",
    "    print(mae_lstm, mse_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_first = dict()\n",
    "steps = {1: 52, 2: 52, 3: 13, 4: 2, 5: 2, 6: 26}\n",
    "for j in range(1, 7):\n",
    "\n",
    "    # обучение\n",
    "    model = models[j]['model']\n",
    "    test  = models[j]['test']\n",
    "    train = models[j]['train']\n",
    "\n",
    "    # прогноз\n",
    "    forecast_df = pd.Series([0]*len(test), index=test.index)\n",
    "    input = X_test[0]\n",
    "    forecast_df[:lookback] = pd.Series(X_test[0], index=test[:lookback].index)\n",
    "    #print(forecast_df[:lookback])\n",
    "    \n",
    "    without_pred = len(forecast_df) - lookback\n",
    "    step = 1\n",
    "    \n",
    "    for i in range(0, len(forecast_df) - lookback, step):\n",
    "        input = forecast_df[i:i+lookback]\n",
    "        input = torch.tensor(input).unsqueeze(0).type(torch.float64)\n",
    "\n",
    "        if without_pred >= lookback:\n",
    "            forecast_df[i+lookback: i+2*lookback] = pd.Series(model(input)[0].detach().numpy(), index=test[i+lookback: i+2*lookback].index)\n",
    "        else:\n",
    "            pred = model(input)[0].detach().numpy()[:without_pred]\n",
    "            forecast_df[i+lookback: i+lookback+without_pred] = pd.Series(pred, index=test[i+lookback: i+lookback+without_pred].index)\n",
    "        without_pred -=step\n",
    "\n",
    "    forecast_df = forecast_df.loc[train_border:]\n",
    "    test = test[train_border:]\n",
    "    res_first[j] = dict()\n",
    "    res_first[j]['forecast'] = forecast_df\n",
    "    res_first[j]['test'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=2, \n",
    "                    subplot_titles = [data_slice[(i % 2) * 3 + i // 2 + 1]['name'] for i in range(6)],\n",
    "                    vertical_spacing = 0.1, horizontal_spacing=0.05)\n",
    "for i in range(6):\n",
    "    fig.add_trace(go.Scatter(x=res[i + 1]['test'].index, y=res[i + 1]['test'], name='Ground truth',\n",
    "                             line=dict(color='Olive'), showlegend=False), row=i%3+1, col=i//3+1)\n",
    "    fig.add_trace(go.Scatter(x=res_first[i + 1]['forecast'].index, y=res_first[i + 1]['forecast'], name='Prediction',\n",
    "                             line=dict(color='mediumvioletred'), showlegend=False), row=i%3+1, col=i//3+1)\n",
    "    fig.add_trace(go.Scatter(x=res[i + 1]['forecast'].index, y=res[i + 1]['forecast'], name='Biased correction',\n",
    "                             line=dict(color='blue'), showlegend=False), row=i%3+1, col=i//3+1)\n",
    "    \n",
    "fig.update_layout(margin=dict(l=20, r=20, t=50, b=20))\n",
    "fig.update_annotations(font_size=12)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE для прогноза на основе LSTL:')\n",
    "for i in range(1, 7):\n",
    "    mae_lstm = mean_absolute_error(res[i]['test'], res_first[i]['forecast'])\n",
    "    print(f'{data_slice[i][\"name\"]}: {mae_lstm}')\n",
    "print()\n",
    "print('MAE для прогноза на основе LSTL cо смещением:')\n",
    "for i in range(1, 7):\n",
    "    mae_lstm = mean_absolute_error(res[i]['test'], res[i]['forecast'])\n",
    "    print(f'{data_slice[i][\"name\"]}: {mae_lstm}')\n",
    "print()\n",
    "print('MSE для прогноза на основе LSTL:')\n",
    "for i in range(1, 7):\n",
    "    mse_lstm = mean_squared_error(res[i]['test'], res_first[i]['forecast'])\n",
    "    print(f'{data_slice[i][\"name\"]}: {mse_lstm}')\n",
    "print()\n",
    "print('MSE для прогноза на основе LSTL cо смещением:')\n",
    "for i in range(1, 7):\n",
    "    mse_lstm = mean_squared_error(res[i]['test'], res[i]['forecast'])\n",
    "    print(f'{data_slice[i][\"name\"]}: {mse_lstm}')\n",
    "print()\n",
    "print('MAPE для прогноза на основе LSTL:')\n",
    "for i in range(1, 7):\n",
    "    mape_lstm = mean_absolute_percentage_error(res[i]['test'], res_first[i]['forecast'])\n",
    "    print(f'{data_slice[i][\"name\"]}: {mape_lstm}')\n",
    "print()\n",
    "print('MAPE для прогноза на основе LSTL cо смещением:')\n",
    "for i in range(1, 7):\n",
    "    mape_lstm = mean_absolute_percentage_error(res[i]['test'], res[i]['forecast'])\n",
    "    print(f'{data_slice[i][\"name\"]}: {mape_lstm}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модельный пример (рис 27-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin = 'Азово-Черноморский бассейн'\n",
    "ports = port_df.loc[port_df['Бассейн'] == basin, 'Название порта'].to_list()\n",
    "ship_type = '60 / General cargo/multi-purpose ship'\n",
    "date_from=date(2022, 1, 1) \n",
    "date_to=date(2022, 12, 31)\n",
    "\n",
    "with Session(engine) as session:\n",
    "    query = session.query(\n",
    "                    PortCall.port_call.label('Port'),\n",
    "                    func.count(PortCall.arrival).label('Number of calls'),\n",
    "                    func.sum(Ship.tonnage).label('Tonnage'),\n",
    "                    func.avg((func.extract('epoch', PortCall.departure)-func.extract('epoch', PortCall.arrival))/ 3600).label('Mean deltatime, h'),\n",
    "                    func.stddev((func.extract('epoch', PortCall.departure)-func.extract('epoch', PortCall.arrival))/ 3600).label('Std deltatime, h'),\n",
    "                    func.max(Ship.length).label('Max length, m'),\n",
    "                    func.max(Ship.width).label('Max width, m')\n",
    "                    ).\\\n",
    "                    join(PortCall, PortCall.ship_id == Ship.id).\\\n",
    "                    join(Port, PortCall.port_id == Port.id).\\\n",
    "                    filter(Port.name.in_(tuple(ports))).\\\n",
    "                    filter(PortCall.arrival >= date_from).\\\n",
    "                    filter(PortCall.arrival <= date_to).\\\n",
    "                    filter(Ship.type == ship_type).\\\n",
    "                    filter(Ship.tonnage != np.NaN).\\\n",
    "                    group_by('Port').\\\n",
    "                    order_by('Tonnage')\n",
    "    res = pd.read_sql(query.statement, query.session.bind)\n",
    "\n",
    "res[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_table = np.zeros((107, 107), dtype=int) # порт отправления, порт прибытия\n",
    "ships = dict() # корабль, нынешний порт\n",
    "\n",
    "date_from=date(2022, 1, 1) \n",
    "date_to=date(2022, 12, 31)\n",
    "\n",
    "with Session(engine) as session:\n",
    "        query = session.query(PortCall.ship_id, PortCall.port_id).\\\n",
    "                        filter(PortCall.arrival >= date_from).\\\n",
    "                        filter(PortCall.arrival <= date_to).\\\n",
    "                        order_by(PortCall.arrival.asc())\n",
    "        \n",
    "        for v in query:\n",
    "            if v[0] in ships.keys():\n",
    "                adj_table[ships[v[0]], v[1]] += 1\n",
    "            ships[v[0]] = v[1]\n",
    "                        \n",
    "G = nx.DiGraph()\n",
    "nodes = [(node, {'name': attr['Название порта'], 'basin': attr['Бассейн']}) for (node, attr) in port_df.to_dict('index').items()]\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "edges = []\n",
    "for i in range(107):\n",
    "    for j in range(107):\n",
    "        if adj_table[i, j] !=0:\n",
    "            G.add_edge(i, j, weight=adj_table[i, j])\n",
    "\n",
    "max_component = max(nx.weakly_connected_components(G), key=len)\n",
    "    \n",
    "G_max_component = nx.Graph()            \n",
    "for node in max_component:  \n",
    "    G_max_component.add_node(node)\n",
    "for (i, j) in list(G.edges):            \n",
    "    if i in max_component and j in max_component:\n",
    "        G_max_component.add_edge(i, j)\n",
    "\n",
    "G = nx.Graph(G_max_component)\n",
    "\n",
    "degree_centrality = nx.centrality.degree_centrality(G)\n",
    "for i in set(range(107)) - set(list(degree_centrality)):\n",
    "    degree_centrality[i] = 0\n",
    "\n",
    "res['Degree centrality'] = 0\n",
    "for i, r in res.iterrows():\n",
    "    res.at[i, 'Degree centrality'] = degree_centrality[port_df[port_df['Название порта'] == r['Port']].index[0]]\n",
    "\n",
    "res = res.set_index('Port')\n",
    "res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = dict()\n",
    "res['MAE'] = 0\n",
    "res['MSE'] = 0\n",
    "\n",
    "for i, r in res.iterrows():\n",
    "    #условия\n",
    "    print(i)\n",
    "    date_from=date(2015, 1, 1) \n",
    "    date_to=date(2023, 12, 31)\n",
    "    df = arrival_dynamics_with_tonnage([i], ship_type, date_from, date_to).tonnage\n",
    "    df_min, df_max = df.min(), df.max()\n",
    "    df = (df-df_min)/(df_max -df_min)\n",
    "    df = df.resample('W').mean()\n",
    "    \n",
    "    # датасет разбитый на train и test\n",
    "    lookback = 52\n",
    "    train_border = date(2020, 12, 31)\n",
    "    train, test = df.loc[:train_border], df.loc[train_border - timedelta(lookback*7):]\n",
    "    X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "    X_test, y_test = create_dataset(test, lookback=lookback)\n",
    "\n",
    "    # обучение\n",
    "    model = model_lstm(X_train, y_train, X_test, y_test, lookback, n_epochs = 500, print_bool=True)\n",
    "    res_model[i] = dict()\n",
    "    res_model[i]['model'] = model\n",
    "\n",
    "    # прогноз\n",
    "    forecast_df = pd.Series([0]*len(test), index=test.index)\n",
    "    input = X_test[0]\n",
    "    forecast_df[:lookback] = pd.Series(X_test[0], index=test[:lookback].index)\n",
    "    without_pred = len(forecast_df) - lookback\n",
    "\n",
    "    for j in range(len(forecast_df) - lookback):\n",
    "        input = forecast_df[j:j+lookback]\n",
    "        input = torch.tensor(input).unsqueeze(0).type(torch.float64)\n",
    "        \n",
    "        if without_pred >= lookback:\n",
    "            forecast_df[j+lookback: j+2*lookback] = pd.Series(model(input)[0].detach().numpy(), index=test[j+lookback: j+2*lookback].index)\n",
    "        else:\n",
    "            pred = model(input)[0].detach().numpy()[:without_pred]\n",
    "            forecast_df[j+lookback: j+lookback+without_pred] = pd.Series(pred, index=test[j+lookback: j+lookback+without_pred].index)\n",
    "        without_pred -=1\n",
    "\n",
    "    forecast_df = forecast_df.loc[train_border:]\n",
    "    test = test[train_border:]\n",
    "    mae = mean_absolute_error(test, forecast_df)\n",
    "    mse = mean_squared_error(test, forecast_df)\n",
    "    res_model[i]['MAE'] = mae\n",
    "    res_model[i]['MSE'] = mse\n",
    "    res.at[i, 'MAE'] = mae\n",
    "    res.at[i,'MSE'] = mse\n",
    "    print(f'MAE: {mae}, MSE: {mse}')\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=forecast_df.index, y=forecast_df, name='prediction'))\n",
    "    fig.add_trace(go.Scatter(x=test.index, y=test, name='test'))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_norm = res.copy()\n",
    "imp_col = {'Number of calls': 1, 'Tonnage': 1, 'Mean deltatime, h': -1, 'Std deltatime, h': -1, 'Max length, m': 1, 'Max width, m': 1, 'Degree centrality': 1, 'MAE': -1, 'MSE': -1}\n",
    "for col in res.columns:\n",
    "    if imp_col[col] == 1:\n",
    "        res_norm[col] = (res_norm[col] - res_norm[col].min())/(res_norm[col].max() - res_norm[col].min())\n",
    "    else:\n",
    "         res_norm[col] = (res_norm[col].max() - res_norm[col])/(res_norm[col].max() - res_norm[col].min())\n",
    "res_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row =  [1/9] * 9 \n",
    "row = [3/25, 2/25, 2/25, 5/25, 0, 0, 1/25, 6/25, 6/25]\n",
    "weight_of_indicators = pd.DataFrame([row], columns=res.columns)\n",
    "weight_of_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_norm['Сonvolution'] = 0\n",
    "for i, col in enumerate(res.columns):\n",
    "    res_norm['Сonvolution'] += (res_norm[col] * row[i])\n",
    "res_norm.sort_values('Сonvolution', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
